# LLM 모델 변경 - qwen3 → qwen2.5

## 배경

- qwen3:8b는 기본 reasoning 모드가 있어서 단순 작업에도 느림
- OllamaLLM에서 reasoning 끄는 옵션이 없음
- 회사명 추출은 단순 작업이라 reasoning 불필요

## 목표

- qwen3:8b → qwen2.5:7b로 변경
- 응답 속도 개선

## 관련 파일

```
apps/news-analyzer/extraction/service.py
```

## 할 것

### 1. 모델명 변경

현재:
```python
llm = OllamaLLM(model="qwen3:8b", base_url=OLLAMA_BASE_URL)
```

변경:
```python
llm = OllamaLLM(model="qwen2.5:7b", base_url=OLLAMA_BASE_URL)
```

### 2. 로컬 테스트

```bash
cd apps/news-analyzer
uvicorn main:app --port 8000

curl -X POST http://localhost:8000/extraction/run \
  -H "Content-Type: application/json" \
  -d '{"news_id": <id>}'
```

### 3. 검증 기준

- 응답 시간 3-5초 이내
- 응답 포맷 동일 (쉼표 구분된 회사명 또는 "없음")

## 안 할 것

- 프롬프트 수정
- 배치 로직 수정
