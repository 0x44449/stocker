# LLM 호출 방식 변경 - ChatOllama → OllamaLLM

## 배경

- ChatOllama가 `/api/chat` 엔드포인트 사용 (대화형)
- 서버 측에서 상태/컨텍스트를 유지하면서 점점 느려지다가 멈추는 것으로 의심
- 일정 건수 처리 후 LLM 응답이 멈추는 현상 발생

## 목표

- ChatOllama → OllamaLLM으로 변경
- `/api/chat` (대화형) → `/api/generate` (단발성)
- 상태 없는 단발성 요청으로 변경

## 관련 파일

```
apps/news-analyzer/extraction/service.py
```

## 할 것

### 1. ChatOllama → OllamaLLM으로 변경

현재:
```python
from langchain_ollama import ChatOllama

def extract_companies(text: str) -> list[str]:
    llm = ChatOllama(model="qwen3:8b", base_url=OLLAMA_BASE_URL, reasoning=False)
    response = llm.invoke(PROMPT_TEMPLATE.format(text=text))
    raw = response.content.strip()
```

변경:
```python
from langchain_ollama import OllamaLLM

def extract_companies(text: str) -> list[str]:
    llm = OllamaLLM(model="qwen3:8b", base_url=OLLAMA_BASE_URL)
    prompt = PROMPT_TEMPLATE.format(text=text)
    raw = llm.invoke(prompt).strip()  # OllamaLLM은 문자열 직접 반환
```

### 2. 로컬 테스트

```bash
cd apps/news-analyzer
uvicorn main:app --port 8000

# pending 목록 조회
curl http://localhost:8000/extraction/pending?limit=5

# extraction 실행
curl -X POST http://localhost:8000/extraction/run \
  -H "Content-Type: application/json" \
  -d '{"news_id": <id>}'
```

로그에서 정상 응답 확인

### 3. 검증 기준

- LLM 호출이 정상적으로 완료되는지
- 응답 포맷이 기존과 동일한지 (쉼표 구분된 회사명 또는 "없음")

## 안 할 것

- 배치 로직 수정
- 프롬프트 수정
